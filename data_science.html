<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="style.css">
</head>
<body>

<ul class="menu">
    <li class="a"><a href="index.html">Home</a></li>
    <li class="a"><a class="active" href="data_science.html">Data Science</a></li>
    <li class="a"><a href="physics.html">Physics</a></li>
    <li class="a"><a href="underwater_photography.html">Underwater Photography</a></li>
</ul>

    <h1>Data Science Projects</h1>


    <hr>
    <h2>Dengue Studies</h2>


<table style="width:100%">
    <tr>
        <td class="left">Notebook Links:</td>
        <td class="right">
        <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/dengue-study.ipynb">Version 1</a>,
        <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/dengue-study-v2.ipynb">Version 2</a>,
        <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/dengue-study-v3.ipynb">Version 3</a>,
        <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/infectious-disease-mc1.ipynb">Infectious MC Version 1</a>
        </td>
    </tr>
</table>

<div class="row">
<div class="fullcolumn">
<p>
    For one of the first Kaggle datasets I looked at after my first Kaggle courses was one on weekly cases of Dengue fever (see figure below) in the cities of San Juan and Iquitos, available <a href="https://www.kaggle.com/datasets/arashnic/epidemy">here</a>.
    I was particularly inspired by the <a href="https://www.kaggle.com/learn/time-series">time-series</a> course, which includes some interesting methods for producing and formatting forecasts.
</p>
    
    <img src="figures/dengue_data.png" alt="Weekly Dengue Infections" style="width:50%; display: block; margin-left: auto; margin-right: auto;">

<p>
    The dataset posted on Kaggle includes several variables to use as features, coming from several sources of meteorological data.
    This includes tempurature and precipitation data from both local measuring stations as well as satellite date. The satellite data also includes humidity measurements and a dew point variable that I assume is computed from the other measurements.
    There is also the satellite NDVI (<a href="https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index">Normalized Difference Vegetation Index</a>), which is a measurement of vegeation density using satellite imaging.
</p>

<p> So far in this project, I have three major versions of the notebook saved, to help track my progress.</p>

    
<p>
    In <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/dengue-study.ipynb">Notebook Version 1</a>, 
    I begin with an investigation of an issue in the data, wherein some of the date information was recorded in a nonsensical way, with the 52nd week of some (but not all) years being assigned to the next year. This was not helped by the fact that another time variable, supposed to give the first day of the week, did not change year-to-year, indicating that it was not referring to any standard week.
    After a fair deal of suffering, I produced a solution to map the dataset's time information into a time index.
</p>

</div>

    <div class="column">
    <p>
        After solving the time information issue, I did some exploratory data analysis, looking at raw plots of the variables.
        I made use of some of the tools from the Time-Series course, including measurements of the correlation of the variable of interest with its past values.
        This includes the Partial Autocorrelation tool, which, for each time-lag, computes the correlation of the target variable with the lagged variable after subtracting off the correlation with the previous time lags.
        The results of this are shown at right. The first point (lag=0) is trivially 1 (a variable is always 100% correlated with itself).
        The high value for the second point (lag = 1) shows that the infection rate is highly correlated with the rate from the previous week.
        The third point and fourth points (lag 2 and 3) indicate an additional dependence on previous values that is more complicated than a simple linear dependence on the previous value.
        Since there are some points beyond the confidence limits up until the 9 week lag, I included up to 10 lags in the set of features for the models to use.    
    </p>
    </div>
    <div class="column">
        <img src="figures/pacf.png" alt="Partial AutoCorrelation" style="width:400px; display: block; margin-left: auto; margin-right: auto;">
    </div>

    <div class="fullcolumn">

    <p>
        I also applied a few feature engineering tools, including principal component analysis and computing features from the derivative of the lagged target.
        I then trained the first models for this project, using the powerful XGBoost Regressor and a tool called a Regressor Chain.
        The models were set up to produce forecasts for for each of the 10 upcoming weeks.
        The results from one of these first models is shown below.
    </p>
    </div>

    <div class="fullcolumn">
        <img src="figures/dengue_model_1.png" alt="Partial AutoCorrelation" style="width:700px; display: block; margin-left: auto; margin-right: auto;">
    </div>

    
    <p>
        In the above plot, the section of the data chosen as the validation range (where the model is not trained) is shown. 
        Each colored line indicates a single forecast, starting on a given week. Overall, these forecasts are certainly not bad
    </p>

    <p>
        In the next notebook, <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/dengue-study-v2.ipynb">version 2</a>, I started applying my first neural networks (outside of courses).
    </p>

    <p>
        One thing that bothered me about these NN results was the fact that the 1 week forecast was not close to value one week before.
        It seems intuitive that this forecast should be strongly impacted by this, and that it should be relatively easy for the model to hit.
        My thinking then was that this could be improved by adding more weight in the loss function to the earlier dates.
        In <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/dengue-study-v3.ipynb">version 3</a>, I implemented such as weighted loss function, using an exponential decay (plus a constant) as weighting.
    </p>

    <div class="fullcolumn">
    <p>
        I was inspired to try modeling the system of repeated disease spread that this data was an example of.
        The spread of infectious disease can be effectively modelled by a system called <a href="https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology">Compartmental models</a>, tracking different populations of susceptibile, infected, and recovered populations and using differential equations to describe their dynamics.
        In some situations, these models can show a single instance of a disease spreading and reaching the maximum number of infected before eventually the entire population is recovered (and immune).

        In the case of recurring, seasonal diseases like Dengue or influenza, a few things must change from the basic models:
        <ul class="b">
            <li class="b">
                Random addition of new infections. Otherwise the infections could got to 0 and never return. New infections could come from travellers or from animal reservoirs.
            </li>
            
            <li class="b">
                Periodicity in some factor and/or a significant introduction of new susceptible people.
            </li>
            <ul>
                <li>If the infectivity changes periodically, that could produce waves of disease. For example, it theorized that flu seasons may be caused by people spending more times together indoors in winter.
                     In the case of Dengue fever, it is plausible that the population of Dengue-spreading mosquitos changes periodically, driven by seasonal changes in temperature, precipitation, and humidity.
                </li>
                <li>
                    Periodicity could also come from waves of infections that fall off when there are no more susceptible people. Then the recovered (immune) population becomes susceptible, probably from a process of waning immunity, until a new wave of infection starts. It is unlikely that this would create a perfectly regular period, though.
                </li>
            </ul>
        </u>

    </p>

    </div> 
    <div class="column">
        
        <p>I implemented some of these ideas in this <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/infectious-disease-mc1.ipynb">notebook</a>
            At right, I've included plot of the weekly infections from different simulations with the same settings.</p>

        <p> I succeeded in one goal of producing artificial data that qualitatively resembles the real-world data.
             Specifically, it has annual waves, but these are not perfectly repetitive, with some years not havinga a significant wave at all.
              I credit this to the noise I added to the rates of increasing infections and recoveries.</p>

        <p>Some possible future improvements to this could include:</p>
        <ul>
            <li> Fine-tuning of the noise and beta parameters to better match the data</li>
            <li> Addition of a process converting recovered population to susceptible. Otherwise, the model leads to a completely immune population.</li>
        </ul>
        <p>Some future use for this model could be to train another model to determine the parameters, such as the parameters for the infectivity and the noise parameters.
            Then I could look at what that model returns when fed the real-world data.
            Could it then be included as part of a more general machine learned model for infectious disease?
        </p>


    </div>

    <div class="column" style="height:500px">
            <img src="figures/mc_infections.png" alt="Monte Carlo Infections Simulation" style="width:450px;height: 500px;">
    </div>
    


 
    </div>
    
    <hr>
    <h2>Company About Us and Clustering</h2>
    
    <table style="width:100%">
        <tr>
            <td class="left">Notebook Links:</td>
            <td class="right">
            <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/company-about/company-about-us-eda.ipynb">Version 1</a>,
            <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/study-of-clustering-metric.ipynb">Clustering Metric Notebook</a>
            </td>
        </tr>
    </table>

    <div class="row">
    
    <div class="fullcolumn">

    <p>
    This project started when I was introduced to the idea of different companies having different kinds of culture and I wondered if there was a lazy way to classify the company cultures automatically.
     The first step was to collect data to use as the input. For this, I decided to use text from the "About Us" pages of the different company websites.
     In order to do this (as well as process the web page I used to produce the list), I learned how to use the <a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> library to parse and search html documents.
     Starting with the 500 companies of the S&P 500, I developed a workflow of scripts to download and parse the websites.
     The latest version captured the about-us information from 215 of 500 websites, an acceptable success rate for my purpose. 
    </p>
    </div>


    <div class="column">
    <p>
        I then took the dataset of company "about us" text and analyzed it in a notebook available
        <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/company-about/company-about-us-eda.ipynb">here</a>. I used some tools I learned in a course on sentiment analysis in 
        Natural Language Processing, including some fun word cloud visualizations.
    </p>

    <p> I then converted the text data into vectors using a count vectorizer.</p>
    </div>

    <div class="column">
        <img src="figures/company_wordcloud.png" alt="Copmany about-us word cloud" style="width:613px;height:289px;">
    </div>

    <div class="fullcolumn">

    <p>
    One issue I found in my study was that the k-means clustering often produced many clusters with only one entry, which means it is probably not a useful clustering.
    This, in addition to the fact that my original motivation was to investigate the validity of the classification of companies into X cultures, led me to develop a metric for 
    the quality/utitility of a clustering.
    </p>

    <p>
        To create such I function, I was inspired by this popular <a href="https://www.youtube.com/watch?v=v68zYyaEmEA">video</a> on choosing the best guesses the NYT Wordle puzzle.
        In the video, the author uses the information theory definition of entropy to measure the information content of a given guess, resulting in CRANE giving most information, on average.
        I did something similar with the clustering, creating a definition for the entropy (information content) of the clustering.
        For the probabilities in the entropy function, I use the relative sizes of the clusters. This is equivalent to asking what the probability that 
        a random entry will be in a given cluster.
    </p>

    </div>

    <div class="column">

    <p>
        I decided to further study and experiment with the clustering metric I produced for the about us study, by producing simulated clusters in an n-dimensional space and running k-means on them. 
        Generating the artificial clusters itself was a fun process, including learning how to use the ortho_group module from scipy to make sure the clusters had random orientations. 
    </p> 
    </div>


    <div class="column">
        <img src="figures/cluster_generated_clusters.png" alt="Clustering study clusters" style="height:290px;">
    </div>



    <div class="column">
    <p>
        I then computed the entropy function I defined previously, and looked at how the entropy changed with the number of clusters in the algorithm.  
    </p>

    <p>
        Notebook link: <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/study-of-clustering-metric.ipynb">Clustering Metric Notebook</a>
    </p>
    
    <p>I may return to this project in the future to apply new tools I am currently learning for NLP. Specifically, using dense vector embeddings for the text instead of one-hot embedding.
         One issue with doing this is that this project has involved unsupervised learning, and the embedding examples I have seen involved training on data with labels.
    </p>

    </div>
    
    <div class="column">
        <img src="figures/cluster_experiment_scaled_entropy.png" alt="Scaled Entropy Ratio" style="height:290px;">
    </div>


    </div>
    <hr>
    <h2>GPU Stock Study</h2>
        
    <table style="width:100%">
        <tr>
          <td class="left">Notebook Link:</td>
          <td class="right"><a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/gpu-company-stock-study-eda-v2.ipynb">Version 2</a></td>
        </tr>
    </table>
    
    <div class="row">

    <div class="column">
    <p>In order to familiarize myself with financial data (security values), I examined some datasets posted on Kaggle, including one on <a href="https://www.kaggle.com/datasets/kapturovalexander/nvidia-amd-intel-asus-msi-share-prices">GPU vendor stocks</a>.
     My goals were to get some practice with working with the measurement of returns (defined as the logarithm of the fractional change in value of an asset over a defined time period) and volatility (defined as the standard deviation of the returns). 
     I also computed several simultaneous correlation functions between different stocks, which can be useful for modelling risk.
     Also included in the notebook are some comparisons to changes in cryptocurrency values, as well as the VIX general market volatility measurement.
    </p>
    </div>
    
    <div class="column">
        <img src="figures/plot_gpu.png" alt="GPU stock study plot" style="width:613px;height:289px;">
    </div>
    
    </div>
    
    <hr>
    <h2>Neural Network and Math Playtime</h2>
    
    <table style="width:100%">
        <tr>
          <td class="left">Notebook Link:</td>
          <td class="right"><a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/neural-network-and-math-playtime.ipynb">Version 1</a></td>
        </tr>
    </table>

    <div class="row">
    <div class="column">
    <p>
        In this mini-project, available <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/neural-network-and-math-playtime.ipynb">here</a>, I experimented with training neural networks on some basic mathematical functions, trying different neuron activation functions.
         I started out with a very basic function, the squaring of a real number, and then moved on to computing the distance in 2 dimensional space between two points.
         
         
         I plan to continue with this notebook to experiment with including neural network layers that directly calculate mathematical functions.
    </p>
    </div>

    <div class="column">
        <img src="figures/plot_math1.png" alt="GPU stock study plot" style="width:300x;height:289px;">
    </div>

    </div>

    <hr>
    <h2>Physics Informed Neural Network</h2>
    
    <table style="width:100%">
        <tr>
          <td class="left">Notebook Link:</td>
          <td class="right"><a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/physics-informed-neural-network-testing.ipynb">Version 1</a></td>
        </tr>
    </table>

    <div class="row">
    <div class="fullcolumn">
    
    <p>
        In this experiment, I started with a Jupyter notebook by Ben Moseley, available <a href="https://github.com/benmoseley/harmonic-oscillator-pinn/blob/main/Harmonic%20oscillator%20PINN.ipynb">here</a>.
         The original notebook showed the modelling of damped harmonic oscillator.
        In my notebook, available <a href="https://github.com/moliver813/Data_Science_Miniprojects/blob/main/physics-informed-neural-network-testing.ipynb">here</a>, modelled the displacement vs time, and showed the benefit of including the differential equation for the damped harmonic oscillator. The equation is included in the model by computing the derivatives of the displacement (velocity and acceleration), pluggin them into the equation, and computing the residual of the model prediction with respect to the equation. This residual was included in the cost function.
        The neural networks trained without the equation were not able to model the systemt well or extrapolate its behavior, while the network with it could. Of course, this is a fairly trivial result, as it mostly just fitting the equation.
    </p>
    </div>
    </div>
 
</body>
</html>